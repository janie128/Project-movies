{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Movie Reviews to Box Office\n",
    "Yen-Ting Chen\n",
    "\n",
    "## Overview\n",
    "Recent studies in predicting movie box office only aim at predicting the opening weekend box office.<sup>1-3</sup>\n",
    "\n",
    "This project aims to find the correlation between movie box office and movie reviews from Amazon.com. Machine learning models are fitted and reviews used to project a movie's box office.\n",
    "\n",
    "There are three main parts to this project, including 1) data scraping and cleaning, 2) feature extraction and preprocessing, and 3) prediction modeling. Parts of the codes used are shown here, while the rest can be accessed on my [Github repository](https://github.com/janie128/Project-movies)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Data Scraping and Cleaning\n",
    "There are two main data sets used in this project. One is the data set of movies and their box offices<sup>4</sup>, and the second is Amazon review data with their metadata<sup>5-7</sup>. \n",
    "\n",
    "#### Box office data\n",
    "Box office data was scraped from the website for all years and movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The following URL and CSS templates rely on the layout of the website. They may change over time. \n",
    "pageUrlTemplate <- \"http://www.boxofficemojo.com/daily/?view=bymovie&yr=%s&page=%d&sort=title&order=ASC\"\n",
    "years <- c(\"2015\", \"2014\", \"2013\", \"2012\", \"2011\",\n",
    "          \"2010\", \"2009\", \"2008\", \"2007\", \"2006\",\n",
    "          \"2005\", \"2004\", \"2003\", \"2002\", \"pre2002\")\n",
    "remainingPageLinkCss <- \"font:nth-child(4) a\" # how many page remaining\n",
    "movieTitleCss <- \"tr+ tr td:nth-child(1) font\" # movie title\n",
    "releaseGrossCss <- \"tr+ tr td:nth-child(4) font\" # release gross\n",
    "releaseDateCss <- \"tr+ tr td:nth-child(5) font\" # release date\n",
    "\n",
    "# Function for parsing the html page and extracting data of interest.\n",
    "extractBoxOfficeFn <- function(page, boxOffice) {\n",
    "  extractedTitles <- html_text(html_nodes(page, movieTitleCss))\n",
    "  extractedReleaseDates <- html_text(html_nodes(page, releaseDateCss))\n",
    "  # Extract release gross, strip \"$\" and \",\", and cast to numeric.\n",
    "  extractedReleaseGrosses <- html_text(html_nodes(page, releaseGrossCss))\n",
    "  extractedReleaseGrosses <- gsub(\"\\\\$\", \"\", extractedReleaseGrosses)\n",
    "  extractedReleaseGrosses <- gsub(\",\", \"\", extractedReleaseGrosses)\n",
    "  extractedReleaseGrosses <- as.numeric(extractedReleaseGrosses)\n",
    "\n",
    "  extractedFromPage <- data.frame(\n",
    "      title = extractedTitles,\n",
    "      gross = extractedReleaseGrosses,\n",
    "      releaseDate = extractedReleaseDates)\n",
    "  return(extractedFromPage)\n",
    "}\n",
    "\n",
    "for (year in years) {\n",
    "  pageUrl <- sprintf(pageUrlTemplate, year, 1);\n",
    "  page <- read_html(pageUrl)\n",
    "  numRemainingPages <- length(html_text(html_nodes(page, remainingPageLinkCss)))\n",
    "\n",
    "  boxOffice <- rbind(boxOffice, extractBoxOfficeFn(page, boxOffice))\n",
    "  for (pageIndex in 1:numRemainingPages) {\n",
    "    pageUrl <- sprintf(pageUrlTemplate, year, pageIndex + 1);\n",
    "    page <- read_html(pageUrl)\n",
    "    boxOffice <- rbind(boxOffice, extractBoxOfficeFn(page, boxOffice))\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was then filtered to include data only after May 1996, to match the Amazon review data period. Initially, the box office amounts were to be adjusted to account for inflation or cultural factors (such as perhaps higher movie-going culture), by fitting a general (upwards) trend regression analysis and factoring it out. However, over the time period, an increase in the percentage of small films (lower-grossing) disrupts any such trend. This is evident in Figure 1, where the percentage of movies grouped by their box office amounts ($log_{10}(gross)$) over the years is shown.  \n",
    "\n",
    "![](./figures/box_office_years.png)\n",
    "**Fig.1** Box office ($log_{10}(gross)$) sorted into 4 buckets and plotted over years. Percentage of highest grossing movies decreases while medium and lower grossing movies are on the rise.\n",
    "\n",
    "Therefore, US inflation rates from 1995-2014 were obtained<sup>8</sup>, and was the only factor used to adjust the gross amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Amazon movie review data\n",
    "Amazon \"Movies & TV\" review data and their metadata files were download in JSON format, which were further parsed and output with a Python script into strict JSON format for parsing in R. Review/metadata files were labeled with product titles as opposed to specifically movie titles, and matching was required between the movie titles from the box office data set and the product titles available in the metadata file. The data matching process involved stripping punctuation, converting to lower case, removing leading \"the\" for a more comprehensive matching, and pattern matching with regular expressions.  \n",
    "  \n",
    "The title-matched data was further cleaned and unnecessary variables removed. As some mismatching was unavoidable with extremely short titles, a subset of this data where the number of characters was greater than 4 was used for the rest of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Feature Extraction and Preprocessing\n",
    "Raw variables of interest available in the review data included the movie reviews, and overall rating (score) for each review. Further meaningful features must be extracted from this data. \n",
    "\n",
    "**(1) Review count (by movie)**: This was extracted using the group_by(), summarize() and count() functions. Figure 2 shows a histogram of the distribution of review counts for all movies. As can be seen, there is a large proportion of movies with review counts below 50 (red line). However, these are considered to be too few to be representative of review quality and are thus discarded for the analysis.  \n",
    "![](./figures/review_count.png)\n",
    "**Fig.2** Distribution of review counts per movie. Data below counts of 50 (red line) are discarded.\n",
    "  \n",
    "**(2) Average score**: This was extracted using the group_by(), summarize() and mean() functions. Figure 3 shows three movie samples and their ratings distribution. It is evident that ratings distributions are significantly different for each movie, and only the average score is not sufficient.  \n",
    "![](./figures/review_ratings.png)\n",
    "**Fig.3** Three samples of movie ratings distribution.  \n",
    "\n",
    "Therefore, rating scores of 1 through 5 and their frequencies are extracted as features.\n",
    "\n",
    "**(3) Rating scores 1 through 5  **\n",
    "\n",
    "**(4) Average word count, word count percentiles (25, 50, 75%)**: Word count for each review was calculated using regex search, then aggregated for each movie. Word count distributions for each movie is likely to be different, so low, mid and high percentiles of the word count distribution for each movie was also calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ---------Reviews word count\n",
    "words <- as.data.frame(sapply(gregexpr(\"\\\\S+\", reviewsTitles$reviewText), length))\n",
    "colnames(words) <- \"wordCount\"\n",
    "reviewsTitles <- cbind(reviewsTitles, words)\n",
    "\n",
    "# ---------Generate table with review word count info including quantiles and average\n",
    "wordCountInfo <- reviewsTitles %>% group_by(movieTitle) %>%\n",
    "  summarize(wordCountLow = quantile(wordCount, probs=0.25), wordCountMid = quantile(wordCount, probs=0.5), \n",
    "            wordCountHigh = quantile(wordCount, probs=0.75), wordCountAvg = round(mean(wordCount),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5) Good/bad word TFIDF (review content word analysis)**: Finally, the review contents were analyzed with natural language processing tokenization, and sentiment analysis performed to produce a set of \"good\" (positive sentiment) and \"bad\" (negative sentiment) TFIDF.  \n",
    "\n",
    "The \"tm\" package was used to perform text mining on each review to obtain a list of occuring words and their frequencies. The usual text cleaning techniques including transformation to lower case, punctuation removal, and extra white space stripping were conducted. However, stopwords were not removed, a decision based on consideration of the amount of information it may be removing on reviews that are relatively short. Dictionaries of good (positive sentiment) and bad (negative sentiment) words were obtained<sup>9-10</sup>, compared to the generated occuring words list, and relevant words found.  \n",
    "Inverse document frequencies for the two dictionaries were calculated:  \n",
    "\n",
    "$$idf(t,D) = log_{10}\\frac{N}{\\{d\\;\\epsilon\\;D:  t\\;\\epsilon\\;d\\}+1}$$  \n",
    "\n",
    "where $N$ is the total number of reviews and $\\{d\\;\\epsilon\\;D:  t\\;\\epsilon\\;d\\}+1$ is the number of reviews where the term $t$ appears. The plus one in the denominator is to avoid cases where the term does not appear in any reviews. This is essentially a weighting for how common a given word is in the set of reviews; the less common it is, the higher its importance and stronger the weighting. \n",
    "\n",
    "Parallel processing with multicores was utilized to speed up the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# Adding IDF to goodDict\n",
    "# -----------------------\n",
    "totalDoc <- length(textAllReviews) # total number of documents\n",
    "\n",
    "goodDictRegex <- goodDict$regex\n",
    "goodDictContain <- numeric(length(goodDictRegex))\n",
    "\n",
    "# Parallel processing & exporting data needed in this segment\n",
    "numCores <- min(3, detectCores() - 1)\n",
    "cluster <- makeCluster(numCores)\n",
    "clusterExport(cluster, \"textAllReviews\")\n",
    "# parLapply used instead of lapply for parallel processing\n",
    "\n",
    "# --Done in chunks to ensure data is saved if anything happens\n",
    "# Count number of documents(reviews) that contain the good word (loops through goodDict)\n",
    "limit <- length(goodDictRegex)\n",
    "begin <- 1\n",
    "end <- 50\n",
    "\n",
    "while (begin <= limit){\n",
    "  realEnd = min(limit, end)\n",
    "  \n",
    "  results <- parLapply(cluster, goodDictRegex[begin:realEnd], function(x) {\n",
    "    sum(grepl(paste(\"\\\\<\", x, \"\\\\>\", sep = \"\"), textAllReviews))\n",
    "  })\n",
    "  goodDictContain[begin:realEnd] <- results\n",
    "  \n",
    "  # For monitoring progress\n",
    "  print(paste(end, \" out of \", limit))\n",
    " \n",
    "  begin <- begin + 50\n",
    "  end <- end + 50\n",
    "}\n",
    "goodDict$contain <- goodDictContain\n",
    "# Turn off parallel\n",
    "stopCluster(cluster)\n",
    "rm(cluster)\n",
    " \n",
    "# Calculate inverse document frequency with denominator +1\n",
    "goodDict$IDF <- lapply(goodDict$contain, function(x) {round(log10(totalDoc/(1 + x)),4)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequently, the term frequencies for each relevant word in a given review were calculated:  \n",
    "\n",
    "$$tf(t,d) = \\frac{f_{t,d}}{max\\{f_{t^{'},d}:\\;t^{'}\\;\\epsilon\\;d\\}}$$\n",
    "\n",
    "where $f_{t,d}$ is the raw frequency of the term $t$ occuring and $max\\{f_{t^{'},d}:\\;t^{'}\\;\\epsilon\\;d\\}$ is the maximum raw frequency of any term in the given review, which accounts for the differing lengths of the reviews. The term frequency-inverse document frequency (TFIDF) $tfidf(t,d,D) = tf(t,d)\\times idf(t,D)$ is then calculated for each word, and the good words and bad words each summed for the review. This process is repeated for all reviews, then averaged for each movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A) Function for generating vector of words in each review (return is class char vector)\n",
    "reviewToWordsFn <- function(text){\n",
    "  text_source <- VectorSource(text)\n",
    "  textCorpus <- Corpus(text_source)\n",
    "  textCorpus <- tm_map(textCorpus, content_transformer(tolower))\n",
    "  textCorpus <- tm_map(textCorpus, removePunctuation)\n",
    "  textCorpus <- tm_map(textCorpus, stripWhitespace)\n",
    "  dtm <- as.matrix(DocumentTermMatrix(textCorpus, control=list(wordLengths=c(1,Inf))))\n",
    "  return(dtm)\n",
    "}\n",
    "\n",
    "# B) Function for generating good OR bad TF-IDF from list of words, returns list of TF-IDF (good or bad)\n",
    "wordsToSentiTfidfFn <- function(listOfWordsFreq, maxTF, sentiDict){\n",
    "  listOfSentiWords <- intersect(listOfWordsFreq$words, sentiDict$words) # char vector of words that are good/bad\n",
    "  listOfSentiWordsTFIDF <- subset(listOfWordsFreq, words %in% listOfSentiWords) # df of good/bad words with their term frequencies\n",
    "  listOfSentiWordsTFIDF$TF <- round(listOfSentiWordsTFIDF$freq/maxTF, 5) # raw TF of word divided by total word count in document\n",
    "  listOfSentiWordsIDF <- subset(sentiDict, words %in% listOfSentiWords)\n",
    "  listOfSentiWordsTFIDF$IDF <- listOfSentiWordsIDF$IDF\n",
    "  rm(listOfSentiWordsIDF)\n",
    "  listOfSentiWordsTFIDF$TFIDF <- mapply(function(x,y) {x*y},\n",
    "                                        listOfSentiWordsTFIDF$TF, listOfSentiWordsTFIDF$IDF) # TF*IDF\n",
    "  return(listOfSentiWordsTFIDF)\n",
    "}\n",
    "\n",
    "# C) Function for generating total TF-IDF from reviews. Calls functions A & B.\n",
    "#    Returns a vector with sum of good words and bad words TFIDF (for each input review).\n",
    "reviewToTfidfFn <- function(text){\n",
    "  listOfWordsFreq <- reviewToWordsFn(text) # matrix of all words (as colname) with their frequencies\n",
    "  maxTF <- max(listOfWordsFreq) # numeric, maximum frequency of above matrix\n",
    "  listOfWords <- colnames(listOfWordsFreq) # char vector of all words\n",
    "  listOfWordsFreq <- cbind(words=listOfWords, freq=listOfWordsFreq[1,]) # matrix of words & freq\n",
    "  row.names(listOfWordsFreq) <- NULL\n",
    "  rm(listOfWords)\n",
    "  listOfWordsFreq <- as.data.frame(listOfWordsFreq, stringsAsFactors = FALSE) # df of words & freq\n",
    "  listOfWordsFreq$freq <- as.numeric(listOfWordsFreq$freq)\n",
    " \n",
    "  # call for good words TF-IDF list\n",
    "  goodTFIDFList <- wordsToSentiTfidfFn(listOfWordsFreq, maxTF, goodDict)\n",
    "  # call for bad words TF-IDF list\n",
    "  badTFIDFList <- wordsToSentiTfidfFn(listOfWordsFreq, maxTF, badDict)\n",
    "  \n",
    "  goodTFIDF <- ifelse(dim(goodTFIDFList)[1]!=0, sum(goodTFIDFList$TFIDF), 0)\n",
    "  badTFIDF <- ifelse(dim(badTFIDFList)[1]!=0, sum(badTFIDFList$TFIDF), 0)\n",
    "\n",
    "  return(c(goodTFIDF, badTFIDF))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Figure 4, it can be seen from the relation between good and bad words TFIDF's to review average scores that the calculated TFIDF's are likely a good indication of the review content.  \n",
    "![](./figures/TFIDF.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. [Predicting Box Office Success a Year in Advance from Ranker Data](http://blog.ranker.com/predicting-box-office-success-ranker-data/#.VotBL_krKUk)\n",
    "2. \"Predicting the Future with Social Media\" S. Asur, B. A. Huberman; *2010 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology*, 2010, pp. 492-499\n",
    "3. \"Early Prediction of Movie Box Office Success Based on Wikipedia Activity Big Data\" M. Mestyan, T. Yasseri, J. Kertesz; *PLoS ONE 8(8): e71226*, 2013\n",
    "4. [Box Office Mojo](http://www.boxofficemojo.com/daily/?view=bymovie&yr=all&sort=title&order=ASC&p=.htm)\n",
    "5. [Dr. Julian McAuley's Amazon product data](http://jmcauley.ucsd.edu/data/amazon/)\n",
    "6. \"Image-based recommendations on styles and substitutes\" J. McAuley, C. Targett, J. Shi, A. van den Hengel; *SIGIR*, 2015\n",
    "7. \"Inferring networks of substitutable and complementary products\" J. McAuley, R. Pandey, J. Leskovec; *Knowledge Discovery and Data Mining*, 2015\n",
    "8. [US Inflation Calculator](http://www.usinflationcalculator.com/inflation/historical-inflation-rates/)\n",
    "9. [Hu and Liu's Opinion Lexicon](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon)\n",
    "10. \"Mining and Summarizing Customer Reviews\" M. Hu, B. Liu; *Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2004)*, Aug 22-25, 2004, Seattle, Washington, USA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
